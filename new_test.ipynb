{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in ./.venv/lib/python3.11/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in ./.venv/lib/python3.11/site-packages (from imblearn) (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.venv/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in ./.venv/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.11/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.46.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from shap) (1.24.3)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from shap) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from shap) (1.2.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from shap) (2.0.1)\n",
      "Collecting tqdm>=4.27.0 (from shap)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>20.9 in ./.venv/lib/python3.11/site-packages (from shap) (24.1)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba (from shap)\n",
      "  Downloading numba-0.60.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->shap)\n",
      "  Downloading llvmlite-0.43.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.11/site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Downloading shap-0.46.0-cp311-cp311-macosx_11_0_arm64.whl (455 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading numba-0.60.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp311-cp311-macosx_11_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, slicer, llvmlite, cloudpickle, numba, shap\n",
      "Successfully installed cloudpickle-3.0.0 llvmlite-0.43.0 numba-0.60.0 shap-0.46.0 slicer-0.0.8 tqdm-4.66.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in target variable y:\n",
      " bank_account\n",
      "0    20212\n",
      "1     3312\n",
      "Name: count, dtype: int64\n",
      "Class distribution after applying SMOTE to training set:\n",
      " bank_account\n",
      "0    16169\n",
      "1    16169\n",
      "Name: count, dtype: int64\n",
      "Logistic Regression MAE: 0.22969884975230795\n",
      "Logistic Regression Accuracy: 0.7703011502476921\n",
      "Logistic Regression Precision: 0.7841685713637674\n",
      "Logistic Regression Recall: 0.7459345406188232\n",
      "Logistic Regression F1 Score: 0.7645539405478707\n",
      "Decision Tree MAE: 0.13126820347324447\n",
      "Decision Tree Accuracy: 0.8687319112808962\n",
      "Decision Tree Precision: 0.8794828814564182\n",
      "Decision Tree Recall: 0.8551590441873682\n",
      "Decision Tree F1 Score: 0.8611954159192653\n",
      "Random Forest MAE: 0.10207678696821379\n",
      "Random Forest Accuracy: 0.8978304821232653\n",
      "Random Forest Precision: 0.8980311642373463\n",
      "Random Forest Recall: 0.8975234713293128\n",
      "Random Forest F1 Score: 0.8943705397418285\n",
      "Unique predictions: [0 1]\n",
      "Random Forest MAE on Test Set: 0.15536663124335812\n",
      "Random Forest Accuracy on Test Set: 0.8446333687566419\n",
      "Random Forest Precision on Test Set: 0.4543046357615894\n",
      "Random Forest Recall on Test Set: 0.5181268882175226\n",
      "Random Forest F1 Score on Test Set: 0.484121383203952\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('./data/Train.csv')\n",
    "\n",
    "# Handle missing values in the target variable\n",
    "df = df.dropna(subset=['bank_account'])\n",
    "\n",
    "# Fill missing values in other columns if necessary\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# One-Hot Encode categorical variables\n",
    "categorical_features = ['country', 'location_type', 'cellphone_access', 'gender_of_respondent', \n",
    "                        'relationship_with_head', 'marital_status', 'education_level', 'job_type']\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "df_encoded = encoder.fit_transform(df[categorical_features]).toarray()  # Convert to dense array\n",
    "\n",
    "# Identify numerical features\n",
    "numerical_features = ['household_size', 'age_of_respondent']\n",
    "\n",
    "# Scale numerical features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_numerical_features = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Combine the scaled numerical features with the one-hot encoded features\n",
    "X = np.hstack((df_encoded, scaled_numerical_features))\n",
    "\n",
    "# Extract the target variable\n",
    "y = df['bank_account'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Verify that there are no NaNs in y\n",
    "assert not np.any(pd.isna(y)), \"Target variable y contains NaN values\"\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "print(\"Class distribution in target variable y:\\n\", y.value_counts())\n",
    "\n",
    "# Split the data before applying SMOTE to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the distribution of the resampled target variable\n",
    "print(\"Class distribution after applying SMOTE to training set:\\n\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Model selection and training\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate models using cross-validation with additional metrics\n",
    "for name, model in models.items():\n",
    "    mae_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=5, scoring='neg_mean_absolute_error')\n",
    "    accuracy_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=5, scoring='precision')\n",
    "    recall_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=5, scoring='recall')\n",
    "    f1_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "    \n",
    "    print(f'{name} MAE: {-mae_scores.mean()}')\n",
    "    print(f'{name} Accuracy: {accuracy_scores.mean()}')\n",
    "    print(f'{name} Precision: {precision_scores.mean()}')\n",
    "    print(f'{name} Recall: {recall_scores.mean()}')\n",
    "    print(f'{name} F1 Score: {f1_scores.mean()}')\n",
    "\n",
    "# Fit the best model (as an example, using RandomForest here)\n",
    "best_model = RandomForestClassifier()\n",
    "best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict and evaluate the final model\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Check if the model is making constant predictions\n",
    "unique_predictions = np.unique(predictions)\n",
    "print(f'Unique predictions: {unique_predictions}')\n",
    "\n",
    "# If the model is making constant predictions, we need to re-evaluate the approach\n",
    "if len(unique_predictions) == 1:\n",
    "    print(\"Model is making constant predictions. Re-evaluate the approach.\")\n",
    "else:\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    \n",
    "    print(f'Random Forest MAE on Test Set: {mae}')\n",
    "    print(f'Random Forest Accuracy on Test Set: {accuracy}')\n",
    "    print(f'Random Forest Precision on Test Set: {precision}')\n",
    "    print(f'Random Forest Recall on Test Set: {recall}')\n",
    "    print(f'Random Forest F1 Score on Test Set: {f1}')\n",
    "\n",
    "    # Model Interpretation using SHAP\n",
    "    import shap\n",
    "\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "    # Save the model\n",
    "    import joblib\n",
    "    joblib.dump(best_model, 'best_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
